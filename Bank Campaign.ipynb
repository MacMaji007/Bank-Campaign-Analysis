{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "701fad48",
   "metadata": {},
   "source": [
    "## Preprocessing of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3059904e",
   "metadata": {},
   "source": [
    "### Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253a7532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d29d751",
   "metadata": {},
   "source": [
    "### Taking input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16ee3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank = pd.read_csv(\"bank-full.csv\")\n",
    "df_bank.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d279f87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17c2482",
   "metadata": {},
   "source": [
    "### Transforming and cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c172bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "\n",
    "df_bank['marital'] = df_bank['marital'].map( {'single': 0, 'married': 1, 'divorced': 2} ).astype(int)\n",
    "df_bank['education'] = df_bank['education'].map( {'unknown': 0, 'primary': 1, 'secondary': 2,'tertiary': 3} ).astype(int)\n",
    "df_bank['default'] = df_bank['default'].map( {'yes': 1, 'no': 0}).astype(int)\n",
    "df_bank['housing'] = df_bank['housing'].map( {'yes': 1, 'no': 0}).astype(int)\n",
    "df_bank['loan'] = df_bank['loan'].map( {'yes': 1, 'no': 0}).astype(int)\n",
    "df_bank['contact'] = df_bank['contact'].map( {'unknown': 0, 'cellular': 1, 'telephone': 2} ).astype(int)\n",
    "df_bank['poutcome'] = df_bank['poutcome'].map( {'unknown': 0, 'failure': 1, 'success': 2,'other': 3} ).astype(int)\n",
    "df_bank['y'] = df_bank['y'].map({'yes':1,'no':0}).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06927b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making job profiles and dropping unnecessary columns\n",
    "\n",
    "def job_pro(x):\n",
    "    if x[\"job\"] == \"retired\" or x['job'] == 'student':\n",
    "        return \"B\"\n",
    "    if x[\"job\"] == \"admin.\" or x['job'] == 'services':\n",
    "        return \"D\"\n",
    "    if x[\"job\"] == \"blue-collar\" or x['job'] == 'management' or x['job'] == 'technician':\n",
    "        return \"C\"\n",
    "    return \"A\"\n",
    "df_bank.apply(lambda x:job_pro(x), axis = 1)\n",
    "df_bank[\"job_profile\"] = df_bank.apply(lambda x:job_pro(x), axis = 1)\n",
    "def jobB(x):\n",
    "    if x[\"job_profile\"] == \"B\":\n",
    "        return 1\n",
    "    return 0\n",
    "df_bank[\"job_B\"] = df_bank.apply(lambda x:jobB(x), axis = 1)\n",
    "df_bank = df_bank.drop(['job','month','job_profile'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1950db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardising the columns balance and duration\n",
    "\n",
    "from sklearn import preprocessing\n",
    "X = df_bank[['balance','duration']]\n",
    "std_scale = preprocessing.StandardScaler().fit(X)\n",
    "X_std = std_scale.transform(X)\n",
    "x_stds = pd.DataFrame(data = X_std)\n",
    "df_bank = pd.concat([df_bank,x_stds],axis=1)\n",
    "df_bank.rename(columns={0:\"Balance\",1:\"Duration\"},inplace=True)\n",
    "df_bank = df_bank.drop(['balance','duration'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb752a9",
   "metadata": {},
   "source": [
    "## After pre-processing of data is finished, we can now proceed with training models on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0865d67",
   "metadata": {},
   "source": [
    "### Splitting Data into training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d7e746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test splitting\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train = df_bank.drop('y',axis=1)\n",
    "Y_train = df_bank['y']\n",
    "X_Train, X_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c6b29f",
   "metadata": {},
   "source": [
    "### Running different models and checking accuracies on training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc07b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LogisticRegression\n",
    "logReg = LogisticRegression()\n",
    "logReg.fit(X_Train,y_train)\n",
    "acc_log = round(logReg.score(X_Train,y_train) * 100, 2)\n",
    "acc_log_test = round(logReg.score(X_test,y_test) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aeb19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(X_Train, y_train)\n",
    "Y_pred = decision_tree.predict(X_Train)\n",
    "acc_decision_tree = round(decision_tree.score(X_Train, y_train) * 100, 2)\n",
    "acc_decision_tree_test = round(decision_tree.score(X_test, y_test) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d114f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "random_forestTest = RandomForestClassifier(n_estimators=100)\n",
    "random_forestTest.fit(X_Train, y_train)\n",
    "Y_predTest = random_forestTest.predict(X_Train)\n",
    "acc_random_forest = round(random_forestTest.score(X_Train, y_train) * 100, 2)\n",
    "acc_random_forest_test = round(random_forestTest.score(X_test, y_test) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693ca8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k- Nearest Neighbor\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn.fit(X_Train, y_train)\n",
    "Y_pred = knn.predict(X_Train)\n",
    "acc_knn = round(knn.score(X_Train, y_train) * 100, 2)\n",
    "acc_knn_test = round(knn.score(X_test, y_test) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef6b72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "gaussian = GaussianNB()\n",
    "gaussian.fit(X_Train, y_train)\n",
    "Y_pred = gaussian.predict(X_Train)\n",
    "acc_gaussian = round(gaussian.score(X_Train, y_train) * 100, 2)\n",
    "acc_gaussian_test = round(gaussian.score(X_test, y_test) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77de535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines\n",
    "svc = SVC()\n",
    "svc.fit(X_Train, y_train)\n",
    "Y_pred = svc.predict(X_Train)\n",
    "acc_svc = round(svc.score(X_Train, y_train) * 100, 2)\n",
    "acc_svc_test = round(svc.score(X_test, y_test) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d003bde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = pd.DataFrame({\n",
    "    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n",
    "              'Random Forest', 'Naive Bayes','Decision Tree'],\n",
    "    'Score': [acc_svc, acc_knn, acc_log, \n",
    "              acc_random_forest, acc_gaussian, acc_decision_tree],\n",
    "    'TestScore': [acc_svc_test,acc_knn_test,acc_log_test,\n",
    "                  acc_random_forest_test,acc_gaussian_test,acc_decision_tree_test]})\n",
    "models.sort_values(by='Score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b566b9f3",
   "metadata": {},
   "source": [
    "### Since Random Forest gave the highest accuracies for both training and testing datasets, proceeding with the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d796eac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forestTest = RandomForestClassifier(random_state=10)\n",
    "random_forestTest.fit(X_train, Y_train)\n",
    "Y_predTest = random_forestTest.predict(X_train)\n",
    "acc_random_forest = round(random_forestTest.score(X_train, Y_train) * 100, 2)\n",
    "acc_random_forest_test = round(random_forestTest.score(X_train, Y_train) * 100, 2)\n",
    "print(\"Training Score =\",acc_random_forest)\n",
    "print(\"Test Score =\",acc_random_forest_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c8e0f4",
   "metadata": {},
   "source": [
    "### Adding a column of predicted probabilities to the dataset against each customer and taking a csv file as output for the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebc2381",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_probTest = random_forestTest.predict_proba(X_train)\n",
    "yy = pd.DataFrame(Y_probTest)\n",
    "yy.rename(columns={0:\"prob Y=0\",1:\"prob Y=1\"},inplace=True)\n",
    "Y_predTest = random_forestTest.predict(X_train)\n",
    "yyy=pd.DataFrame(Y_predTest)\n",
    "yyy.rename(columns={0:\"pred Y\"},inplace=True)\n",
    "df_bank = pd.concat([df_bank,yyy,yy],axis=1)\n",
    "df_bank.to_csv(\"bank_predt_RF.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e95ea5",
   "metadata": {},
   "source": [
    "### After this, we sorted the predicted probabilities column and got the list of the customers with highest chances of buying the term policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
